{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Australia Fire Prediction Modeling\n",
    "\n",
    "This notebook demonstrates data preparation, model training, and evaluation for predicting Australian fire characteristics using satellite data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in c:\\users\\voqua\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.38.24)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\voqua\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.10.3)\n",
      "Requirement already satisfied: seaborn in c:\\users\\voqua\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\voqua\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\voqua\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\voqua\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.6)\n",
      "Requirement already satisfied: kagglehub in c:\\users\\voqua\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.3.12)\n",
      "Requirement already satisfied: botocore<1.39.0,>=1.38.24 in c:\\users\\voqua\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from boto3) (1.38.24)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\voqua\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in c:\\users\\voqua\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from boto3) (0.13.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\voqua\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\voqua\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\voqua\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (4.58.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\voqua\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\voqua\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\voqua\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\voqua\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\voqua\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\voqua\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\voqua\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\voqua\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\voqua\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\voqua\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\voqua\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from kagglehub) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\voqua\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from kagglehub) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\voqua\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from kagglehub) (4.67.1)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in c:\\users\\voqua\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from botocore<1.39.0,>=1.38.24->boto3) (2.4.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\voqua\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\voqua\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->kagglehub) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\voqua\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->kagglehub) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\voqua\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->kagglehub) (2025.4.26)\n",
      "Requirement already satisfied: colorama in c:\\users\\voqua\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm->kagglehub) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\voqua\\AppData\\Local\\Programs\\Python\\Python311\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install boto3 matplotlib seaborn scikit-learn pandas numpy kagglehub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Acquisition and Preparation\n",
    "\n",
    "First, we'll set up AWS connections and retrieve data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "# Add the project root to the path so we can import the src modules\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "# Import our utility modules\n",
    "from src.utils.aws_utils import download_from_s3, upload_model_to_s3, ensure_bucket_exists\n",
    "from src.utils.create_dataset import clean_fire_data\n",
    "from src.utils.generate_features import generate_fire_features\n",
    "from src.utils.train_model import simple_parameter_tuning\n",
    "from src.utils.score_model import evaluate_model\n",
    "from src.utils.analysis import save_figures, plot_feature_importance\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# S3 client\n",
    "s3_client = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Download Data from Kaggle\n",
    "\n",
    "We'll download the \"Fires from Space\" Australia and New Zealand dataset from Kaggle using kagglehub,\n",
    "then upload the raw data to our S3 bucket for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 22:43:04,118 - __main__ - INFO - Downloading fire dataset from Kaggle...\n",
      "2025-05-29 22:43:04,690 - __main__ - INFO - Dataset downloaded to: C:\\Users\\voqua\\.cache\\kagglehub\\datasets\\carlosparadis\\fires-from-space-australia-and-new-zeland\\versions\\1\n",
      "2025-05-29 22:43:04,693 - __main__ - INFO - Found 4 CSV files in the dataset\n",
      "2025-05-29 22:43:04,694 - __main__ - INFO - Using fire data file: C:\\Users\\voqua\\.cache\\kagglehub\\datasets\\carlosparadis\\fires-from-space-australia-and-new-zeland\\versions\\1\\fire_nrt_M6_96619.csv\n",
      "2025-05-29 22:43:04,694 - src.utils.aws_utils - INFO - Checking if bucket 'fire-prediction-data' exists\n",
      "2025-05-29 22:43:05,177 - src.utils.aws_utils - INFO - Bucket 'fire-prediction-data' doesn't exist, creating it in region 'us-east-2'\n",
      "2025-05-29 22:43:05,726 - src.utils.aws_utils - INFO - Successfully created bucket 'fire-prediction-data'\n",
      "2025-05-29 22:43:05,735 - __main__ - INFO - Uploading raw data to S3: s3://fire-prediction-data/data/fire_nrt.csv\n",
      "2025-05-29 22:43:19,041 - __main__ - INFO - Successfully uploaded raw data to S3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of the fire data:\n",
      "   latitude  longitude  brightness  scan  track    acq_date  acq_time  \\\n",
      "0   -14.281    143.636       323.9   1.7    1.3  2019-10-01        25   \n",
      "1   -14.284    143.532       343.5   1.7    1.3  2019-10-01        25   \n",
      "2   -14.302    143.706       320.2   1.7    1.3  2019-10-01        25   \n",
      "3   -14.283    143.652       320.4   1.7    1.3  2019-10-01        25   \n",
      "4   -14.285    143.521       349.4   1.7    1.3  2019-10-01        25   \n",
      "\n",
      "  satellite instrument  confidence version  bright_t31    frp daynight  \n",
      "0     Terra      MODIS          70  6.0NRT       302.3   26.8        D  \n",
      "1     Terra      MODIS          90  6.0NRT       306.3   84.3        D  \n",
      "2     Terra      MODIS          30  6.0NRT       305.0   14.1        D  \n",
      "3     Terra      MODIS          57  6.0NRT       303.3   18.4        D  \n",
      "4     Terra      MODIS          94  6.0NRT       304.7  110.7        D  \n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Create data directories\n",
    "local_data_dir = Path('../data')\n",
    "local_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "raw_data_dir = local_data_dir / 'raw'\n",
    "raw_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Download dataset from Kaggle\n",
    "logger.info(\"Downloading fire dataset from Kaggle...\")\n",
    "try:\n",
    "    # Download latest version of the dataset\n",
    "    dataset_path = kagglehub.dataset_download(\"carlosparadis/fires-from-space-australia-and-new-zeland\")\n",
    "    logger.info(f\"Dataset downloaded to: {dataset_path}\")\n",
    "    \n",
    "    # List all downloaded CSV files\n",
    "    csv_files = glob.glob(os.path.join(dataset_path, \"*.csv\"))\n",
    "    logger.info(f\"Found {len(csv_files)} CSV files in the dataset\")\n",
    "    \n",
    "    if csv_files:\n",
    "        # Use the first CSV file or look for a specific one\n",
    "        # You might want to choose a specific file based on your needs\n",
    "        fire_data_file = None\n",
    "        for file in csv_files:\n",
    "            if 'fire_nrt' in file.lower():\n",
    "                fire_data_file = file\n",
    "                break\n",
    "        \n",
    "        if not fire_data_file and csv_files:\n",
    "            fire_data_file = csv_files[0]\n",
    "            \n",
    "        if fire_data_file:\n",
    "            logger.info(f\"Using fire data file: {fire_data_file}\")\n",
    "            \n",
    "            # Upload the raw file to S3\n",
    "            bucket_name = 'fire-prediction-data'  # Same bucket defined earlier\n",
    "            aws_region = 'us-east-2'  # Same region defined earlier\n",
    "            \n",
    "            # Make sure the bucket exists\n",
    "            ensure_bucket_exists(bucket_name, aws_region, True)\n",
    "            \n",
    "            # Define S3 key for the raw data\n",
    "            s3_key = 'data/fire_nrt.csv'\n",
    "            \n",
    "            # Upload the file to S3\n",
    "            try:\n",
    "                logger.info(f\"Uploading raw data to S3: s3://{bucket_name}/{s3_key}\")\n",
    "                s3_client.upload_file(fire_data_file, bucket_name, s3_key)\n",
    "                logger.info(f\"Successfully uploaded raw data to S3\")\n",
    "                \n",
    "                # Update the file path for future reference\n",
    "                file_path = fire_data_file\n",
    "                \n",
    "                # Create a small sample of the data to display\n",
    "                df_sample = pd.read_csv(fire_data_file, nrows=5)\n",
    "                print(\"\\nSample of the fire data:\")\n",
    "                print(df_sample)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error uploading to S3: {e}\")\n",
    "        else:\n",
    "            logger.error(\"No suitable CSV file found in the downloaded dataset\")\n",
    "    else:\n",
    "        logger.error(\"No CSV files found in the downloaded dataset\")\n",
    "        \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error downloading dataset from Kaggle: {e}\")\n",
    "    logger.info(\"If you encounter authentication issues, make sure your Kaggle API credentials are set up.\")\n",
    "    logger.info(\"You can download the dataset manually from: https://www.kaggle.com/datasets/carlosparadis/fires-from-space-australia-and-new-zeland\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 22:43:19,095 - src.utils.aws_utils - INFO - Checking if bucket 'fire-prediction-data' exists\n",
      "2025-05-29 22:43:19,522 - src.utils.aws_utils - INFO - Bucket 'fire-prediction-data' exists\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# S3 bucket details - these will be used by our pipeline too\n",
    "bucket_name = 'fire-prediction-data'  # Choose a globally unique bucket name\n",
    "aws_region = 'us-east-2'  # Specify your preferred region\n",
    "file_path = '../data/fire_nrt.csv'  # Path to your local file\n",
    "s3_key = 'data/fire_nrt.csv'  # The path/name the file will have in S3\n",
    "\n",
    "# This config structure matches what we'll use in our YAML config\n",
    "fire_config = {\n",
    "    \"run_config\": {\n",
    "        \"name\": \"fire-prediction-model\",\n",
    "        \"author\": \"Your Name\",\n",
    "        \"version\": \"1.0\",\n",
    "        \"description\": \"Predicts fire brightness based on satellite data\",\n",
    "        \"dependencies\": \"requirements.txt\",\n",
    "        \"data_source\": f\"s3://{bucket_name}/{s3_key}\",\n",
    "        \"output\": \"artifacts\"\n",
    "    },\n",
    "    \"aws\": {\n",
    "        \"bucket_name\": bucket_name,\n",
    "        \"region\": aws_region,\n",
    "        \"prefix\": \"fire-experiments\",\n",
    "        \"create_bucket_if_missing\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "# Check if bucket exists and create it if necessary - this will be handled by aws_utils.py in the pipeline\n",
    "ensure_bucket_exists(bucket_name, aws_region, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 22:43:19,541 - src.utils.aws_utils - INFO - Downloading data/fire_nrt.csv from bucket fire-prediction-data to ../data/fire_nrt.csv\n",
      "2025-05-29 22:43:24,623 - src.utils.aws_utils - INFO - Successfully downloaded file to ../data/fire_nrt.csv\n",
      "2025-05-29 22:43:24,739 - __main__ - INFO - Loaded data with 183593 rows and 14 columns\n",
      "2025-05-29 22:43:24,739 - src.utils.create_dataset - INFO - Cleaning fire data\n",
      "2025-05-29 22:43:24,770 - src.utils.create_dataset - INFO - After dropping missing values: 183593 rows\n",
      "2025-05-29 22:43:24,789 - src.utils.create_dataset - INFO - After geographic filtering: 183591 rows\n",
      "2025-05-29 22:43:24,805 - src.utils.create_dataset - INFO - After outlier removal: 179965 rows\n",
      "2025-05-29 22:43:24,805 - src.utils.create_dataset - INFO - Data cleaning complete. Rows remaining: 179965\n",
      "2025-05-29 22:43:25,438 - __main__ - INFO - Cleaned data saved to ..\\data\\fire_nrt_cleaned.csv\n",
      "2025-05-29 22:43:35,005 - __main__ - INFO - Uploaded cleaned data to s3://fire-prediction-data/data/fire_nrt_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "# Main data preparation\n",
    "# S3 configuration\n",
    "local_data_dir = Path('../data')\n",
    "local_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "local_data_path = local_data_dir / 'fire_nrt_cleaned.csv'\n",
    "\n",
    "# Try to load existing cleaned data first\n",
    "if os.path.exists(local_data_path):\n",
    "    logger.info(f\"Loading existing cleaned data from {local_data_path}\")\n",
    "    df_clean = pd.read_csv(local_data_path)\n",
    "    if 'acq_date' in df_clean.columns:\n",
    "        df_clean['acq_date'] = pd.to_datetime(df_clean['acq_date'])\n",
    "else:\n",
    "    # Download file from S3\n",
    "    temp_raw_path = '../data/fire_nrt.csv'\n",
    "    if download_from_s3(bucket_name, s3_key, temp_raw_path):\n",
    "        # Load the data\n",
    "        try:\n",
    "            df = pd.read_csv(temp_raw_path)\n",
    "            logger.info(f\"Loaded data with {len(df)} rows and {len(df.columns)} columns\")\n",
    "            \n",
    "            # Clean the data using our module function\n",
    "            df_clean = clean_fire_data(df)\n",
    "            \n",
    "            # Save cleaned data\n",
    "            df_clean.to_csv(local_data_path, index=False)\n",
    "            logger.info(f\"Cleaned data saved to {local_data_path}\")\n",
    "            \n",
    "            # Optionally upload cleaned data back to S3\n",
    "            clean_s3_key = 'data/fire_nrt_cleaned.csv'\n",
    "            s3_client.upload_file(str(local_data_path), bucket_name, clean_s3_key)\n",
    "            logger.info(f\"Uploaded cleaned data to s3://{bucket_name}/{clean_s3_key}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing the data: {e}\")\n",
    "    else:\n",
    "        logger.error(\"Failed to download the file from S3. Check your credentials and bucket name.\")\n",
    "        \n",
    "# Make a copy for our modeling work\n",
    "fire_data = df_clean.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Selection and Data Preparation\n",
    "\n",
    "Select features and prepare data for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 22:43:35,025 - src.utils.generate_features - INFO - Generating features for fire prediction model\n",
      "2025-05-29 22:43:35,030 - src.utils.generate_features - INFO - Adding derived features\n",
      "2025-05-29 22:43:35,033 - src.utils.generate_features - INFO - Added frp_per_area feature\n",
      "2025-05-29 22:43:35,035 - src.utils.generate_features - INFO - Added temperature_diff feature\n",
      "2025-05-29 22:43:35,035 - src.utils.generate_features - INFO - Feature generation complete. Total features: 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature statistics:\n",
      "            latitude      longitude           scan          track  \\\n",
      "count  179965.000000  179965.000000  179965.000000  179965.000000   \n",
      "mean      -26.977825     141.933228       1.609390       1.209982   \n",
      "std         8.177908      11.018903       0.815002       0.248601   \n",
      "min       -43.116000     113.458000       1.000000       1.000000   \n",
      "25%       -33.038000     131.569000       1.000000       1.000000   \n",
      "50%       -30.060000     147.868000       1.300000       1.100000   \n",
      "75%       -17.695000     150.675000       1.900000       1.300000   \n",
      "max       -10.101000     153.477000       4.800000       2.000000   \n",
      "\n",
      "          bright_t31            frp     confidence   frp_per_area  \\\n",
      "count  179965.000000  179965.000000  179965.000000  179965.000000   \n",
      "mean      302.543666      75.477595      74.505104      38.558686   \n",
      "std        11.415937     128.434577      25.041051      54.093149   \n",
      "min       265.700000       0.000000       0.000000       0.000000   \n",
      "25%       293.700000      18.000000      58.000000      11.363636   \n",
      "50%       301.700000      35.700000      81.000000      19.940476   \n",
      "75%       311.100000      78.700000      98.000000      40.303030   \n",
      "max       400.100000    3781.600000     100.000000     460.909091   \n",
      "\n",
      "       temperature_diff  \n",
      "count     179965.000000  \n",
      "mean          34.103741  \n",
      "std           18.313128  \n",
      "min           10.000000  \n",
      "25%           21.500000  \n",
      "50%           29.200000  \n",
      "75%           41.600000  \n",
      "max          147.700000  \n",
      "\n",
      "First few rows of features:\n",
      "   latitude  longitude  scan  track  bright_t31    frp  confidence  \\\n",
      "0   -14.281    143.636   1.7    1.3       302.3   26.8          70   \n",
      "1   -14.284    143.532   1.7    1.3       306.3   84.3          90   \n",
      "2   -14.302    143.706   1.7    1.3       305.0   14.1          30   \n",
      "3   -14.283    143.652   1.7    1.3       303.3   18.4          57   \n",
      "4   -14.285    143.521   1.7    1.3       304.7  110.7          94   \n",
      "\n",
      "   frp_per_area  temperature_diff  \n",
      "0     12.126697              21.6  \n",
      "1     38.144796              37.2  \n",
      "2      6.380090              15.2  \n",
      "3      8.325792              17.1  \n",
      "4     50.090498              44.7  \n",
      "\n",
      "Training set size: 134973 samples\n",
      "Validation set size: 44992 samples\n"
     ]
    }
   ],
   "source": [
    "# Define feature generation config\n",
    "feature_config = {\n",
    "    \"target_column\": \"brightness\",\n",
    "    \"feature_columns\": ['latitude', 'longitude', 'scan', 'track', 'bright_t31', 'frp', 'confidence'],\n",
    "    \"derived_features\": [\"frp_per_area\", \"temperature_diff\"]\n",
    "}\n",
    "\n",
    "# Generate features using our module function\n",
    "fire_features = generate_fire_features(fire_data, feature_config)\n",
    "\n",
    "# Define target and features for modeling\n",
    "y = fire_features.brightness\n",
    "X = fire_features[feature_config[\"feature_columns\"] + \n",
    "                ['frp_per_area', 'temperature_diff'] if 'temperature_diff' in fire_features.columns else ['frp_per_area']]\n",
    "\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "# Review data\n",
    "print(\"Feature statistics:\")\n",
    "print(X.describe())\n",
    "\n",
    "print(\"\\nFirst few rows of features:\")\n",
    "print(X.head())\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n",
    "\n",
    "print(f\"\\nTraining set size: {len(train_X)} samples\")\n",
    "print(f\"Validation set size: {len(val_X)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training and Evaluation\n",
    "\n",
    "### 3.1 Linear Regression Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 22:43:35,159 - src.utils.score_model - INFO - Model Evaluation Results:\n",
      "2025-05-29 22:43:35,159 - src.utils.score_model - INFO - MAE: 0.0000\n",
      "2025-05-29 22:43:35,160 - src.utils.score_model - INFO - RMSE: 0.0000\n",
      "2025-05-29 22:43:35,160 - src.utils.score_model - INFO - R²: 1.0000\n",
      "2025-05-29 22:43:35,160 - src.utils.score_model - INFO - Explained Variance: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Linear Regression baseline model...\n",
      "\n",
      "Feature Coefficients:\n",
      "            feature   coefficient\n",
      "8  temperature_diff  1.000000e+00\n",
      "4        bright_t31  1.000000e+00\n",
      "3             track  4.340309e-14\n",
      "7      frp_per_area  1.034601e-15\n",
      "6        confidence  9.385980e-16\n",
      "0          latitude  3.861888e-16\n",
      "5               frp  1.390482e-16\n",
      "1         longitude -1.762479e-15\n",
      "2              scan -2.549055e-14\n",
      "Linear Regression model saved to ..\\models\\linear_regression_baseline.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\voqua\\AppData\\Local\\Temp\\ipykernel_11036\\4138642131.py:30: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "2025-05-29 22:43:35,655 - src.utils.aws_utils - INFO - Successfully uploaded model to s3://fire-prediction-data/models/linear_regression_baseline.pkl\n"
     ]
    }
   ],
   "source": [
    "def train_linear_regression_baseline():\n",
    "    print(\"Training Linear Regression baseline model...\")\n",
    "    \n",
    "    # Create and train the model\n",
    "    lr_model = LinearRegression()\n",
    "    lr_model.fit(train_X, train_y)\n",
    "    \n",
    "    # Use our module function to evaluate the model\n",
    "    metrics = evaluate_model(lr_model, val_X, val_y)\n",
    "    \n",
    "    # Get feature coefficients\n",
    "    coefficients = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'coefficient': lr_model.coef_\n",
    "    }).sort_values('coefficient', ascending=False)\n",
    "    \n",
    "    print(\"\\nFeature Coefficients:\")\n",
    "    print(coefficients)\n",
    "    \n",
    "    # Plot feature coefficients\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='coefficient', y='feature', data=coefficients)\n",
    "    plt.title('Linear Regression Feature Coefficients')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Create figures directory if it doesn't exist\n",
    "    figures_dir = Path('../figures')\n",
    "    figures_dir.mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(figures_dir / 'lr_feature_coefficients.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Save the model\n",
    "    model_dir = Path('../models')\n",
    "    model_dir.mkdir(parents=True, exist_ok=True)\n",
    "    model_path = model_dir / 'linear_regression_baseline.pkl'\n",
    "    \n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump(lr_model, f)\n",
    "    \n",
    "    print(f\"Linear Regression model saved to {model_path}\")\n",
    "    \n",
    "    # Upload model to S3 using our module function\n",
    "    upload_model_to_s3(model_path, 'linear_regression_baseline.pkl', bucket_name, aws_region)\n",
    "    \n",
    "    return lr_model, metrics\n",
    "\n",
    "# Train linear regression model\n",
    "lr_model, lr_metrics = train_linear_regression_baseline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Fine-tune Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 22:43:35,663 - src.utils.train_model - INFO - Performing simple parameter tuning for max_leaf_nodes\n",
      "2025-05-29 22:43:35,668 - src.utils.train_model - INFO - Testing 7 candidate values for max_leaf_nodes\n",
      "2025-05-29 22:43:38,954 - src.utils.train_model - INFO - max_leaf_nodes=5000, MAE=0.5561\n",
      "2025-05-29 22:43:42,521 - src.utils.train_model - INFO - max_leaf_nodes=10000, MAE=0.4788\n",
      "2025-05-29 22:43:46,287 - src.utils.train_model - INFO - max_leaf_nodes=15000, MAE=0.4448\n",
      "2025-05-29 22:43:50,162 - src.utils.train_model - INFO - max_leaf_nodes=20000, MAE=0.4267\n",
      "2025-05-29 22:43:54,137 - src.utils.train_model - INFO - max_leaf_nodes=25000, MAE=0.4173\n",
      "2025-05-29 22:43:58,268 - src.utils.train_model - INFO - max_leaf_nodes=30000, MAE=0.4121\n",
      "2025-05-29 22:44:02,986 - src.utils.train_model - INFO - max_leaf_nodes=50000, MAE=0.4076\n",
      "2025-05-29 22:44:02,988 - src.utils.train_model - INFO - Best max_leaf_nodes: 50000\n",
      "2025-05-29 22:44:04,154 - src.utils.score_model - INFO - Model Evaluation Results:\n",
      "2025-05-29 22:44:04,154 - src.utils.score_model - INFO - MAE: 0.3741\n",
      "2025-05-29 22:44:04,154 - src.utils.score_model - INFO - RMSE: 0.7741\n",
      "2025-05-29 22:44:04,154 - src.utils.score_model - INFO - R²: 0.9989\n",
      "2025-05-29 22:44:04,154 - src.utils.score_model - INFO - Explained Variance: 0.9989\n",
      "2025-05-29 22:44:04,230 - src.utils.analysis - INFO - Feature importance plot saved to ..\\figures\\dt_feature_importance.png\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model with best max_leaf_nodes:\n",
      "Best model saved to ..\\models\\best_decision_tree.pkl\n"
     ]
    }
   ],
   "source": [
    "# Use our module function for parameter tuning\n",
    "best_leaf_nodes = simple_parameter_tuning(train_X, train_y)\n",
    "\n",
    "# Create and train a model with the best max_leaf_nodes\n",
    "best_simple_model = DecisionTreeRegressor(max_leaf_nodes=best_leaf_nodes, random_state=1)\n",
    "best_simple_model.fit(train_X, train_y)\n",
    "\n",
    "print(\"\\nEvaluating model with best max_leaf_nodes:\")\n",
    "simple_metrics = evaluate_model(best_simple_model, val_X, val_y)\n",
    "\n",
    "# Plot feature importance using our module function\n",
    "figures_dir = Path('../figures')\n",
    "figures_dir.mkdir(parents=True, exist_ok=True)\n",
    "plot_feature_importance(best_simple_model, feature_names, figures_dir / 'dt_feature_importance.png')\n",
    "\n",
    "# Save the model\n",
    "model_dir = Path('../models')\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(model_dir / 'best_decision_tree.pkl', 'wb') as f:\n",
    "    pickle.dump(best_simple_model, f)\n",
    "\n",
    "print(f\"Best model saved to {model_dir / 'best_decision_tree.pkl'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Comparison and Upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Comparison:\n",
      "Linear Regression MAE: 0.0000\n",
      "Decision Tree MAE: 0.3741\n",
      "Improvement with Decision Tree: -648317472319083.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-29 22:44:09,023 - src.utils.aws_utils - INFO - Successfully uploaded model to s3://fire-prediction-data/models/best_decision_tree.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare the models\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(f\"Linear Regression MAE: {lr_metrics['mae']:.4f}\")\n",
    "print(f\"Decision Tree MAE: {simple_metrics['mae']:.4f}\")\n",
    "print(f\"Improvement with Decision Tree: {(lr_metrics['mae'] - simple_metrics['mae']) / lr_metrics['mae'] * 100:.2f}%\")\n",
    "\n",
    "# Upload the best decision tree model using our module function\n",
    "upload_model_to_s3(Path('../models/best_decision_tree.pkl'), 'best_decision_tree.pkl', bucket_name, aws_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "In this notebook, we have:\n",
    "1. Acquired and cleaned Australian fire data\n",
    "2. Trained baseline linear regression and decision tree models\n",
    "3. Fine-tuned the decision tree model with cross-validation\n",
    "4. Compared model performance and identified the best model\n",
    "5. Saved and uploaded the models to S3 for deployment\n",
    "\n",
    "The decision tree model outperformed the linear regression baseline, with particular importance placed on features like brightness temperature, fire radiative power, and location coordinates.\n",
    "\n",
    "This notebook serves as a prototype for our pipeline implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Creating Configuration for Multi-Model Pipeline\n",
    "\n",
    "Now we'll create a configuration file that supports training both Linear Regression and Decision Tree models in our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-model configuration saved to ..\\config\\fire-prediction-multi-model-config.yaml\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "# Define a comprehensive configuration that supports multiple models\n",
    "multi_model_config = {\n",
    "    \"run_config\": {\n",
    "        \"name\": \"fire-prediction-multi-model\",\n",
    "        \"author\": \"Your Name\",\n",
    "        \"version\": \"1.0\",\n",
    "        \"description\": \"Predicts fire brightness using multiple regression models\",\n",
    "        \"dependencies\": \"requirements.txt\",\n",
    "        \"data_source\": f\"s3://{bucket_name}/{s3_key}\",\n",
    "        \"output\": \"artifacts\"\n",
    "    },\n",
    "    \"create_dataset\": {\n",
    "        \"critical_columns\": [\n",
    "            \"latitude\", \"longitude\", \"brightness\", \"scan\", \"track\", \n",
    "            \"bright_t31\", \"frp\", \"confidence\"\n",
    "        ],\n",
    "        \"region_filter\": {\n",
    "            \"enabled\": True,\n",
    "            \"min_latitude\": -44,\n",
    "            \"max_latitude\": -10,\n",
    "            \"min_longitude\": 112,\n",
    "            \"max_longitude\": 154\n",
    "        },\n",
    "        \"outlier_removal\": {\n",
    "            \"enabled\": True,\n",
    "            \"column\": \"brightness\",\n",
    "            \"std_threshold\": 3\n",
    "        },\n",
    "        \"target_column\": \"brightness\"\n",
    "    },\n",
    "    \"generate_features\": {\n",
    "        \"target_column\": \"brightness\",\n",
    "        \"feature_columns\": [\n",
    "            \"latitude\", \"longitude\", \"scan\", \"track\", \n",
    "            \"bright_t31\", \"frp\", \"confidence\"\n",
    "        ],\n",
    "        \"derived_features\": [\n",
    "            \"frp_per_area\", \n",
    "            \"temperature_diff\"\n",
    "        ],\n",
    "        \"transformations\": {\n",
    "            \"log_transform\": {\n",
    "                \"log_frp\": \"frp\"\n",
    "            },\n",
    "            \"normalize\": {\n",
    "                \"enabled\": True,\n",
    "                \"method\": \"min_max\",\n",
    "                \"columns\": [\"latitude\", \"longitude\", \"scan\", \"track\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"train_model\": {\n",
    "        \"models\": [\n",
    "            {\n",
    "                \"name\": \"linear_regression\",\n",
    "                \"type\": \"LinearRegression\",\n",
    "                \"hyperparameters\": {},\n",
    "                \"is_default\": False\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"decision_tree\",\n",
    "                \"type\": \"DecisionTreeRegressor\",\n",
    "                \"hyperparameters\": {\n",
    "                    \"max_leaf_nodes\": best_leaf_nodes,  # Use the best value from our tuning\n",
    "                    \"random_state\": 42\n",
    "                },\n",
    "                \"is_default\": True  # This will be used for default predictions\n",
    "            }\n",
    "        ],\n",
    "        \"test_size\": 0.3,\n",
    "        \"random_state\": 42,\n",
    "        \"target_column\": \"brightness\",\n",
    "        \"initial_features\": [\n",
    "            \"latitude\", \"longitude\", \"scan\", \"track\", \n",
    "            \"bright_t31\", \"frp\", \"confidence\", \n",
    "            \"frp_per_area\", \"temperature_diff\"\n",
    "        ]\n",
    "    },\n",
    "    \"score_model\": {\n",
    "        \"metrics\": [\n",
    "            \"mae\",\n",
    "            \"rmse\",\n",
    "            \"r2\",\n",
    "            \"explained_variance\"\n",
    "        ]\n",
    "    },\n",
    "    \"evaluate_performance\": {\n",
    "        \"visualization\": [\n",
    "            \"feature_importance\",\n",
    "            \"predictions_vs_actual\",\n",
    "            \"residuals_plot\",\n",
    "            \"model_comparison\"  # New visualization for comparing models\n",
    "        ],\n",
    "        \"save_format\": \"png\"\n",
    "    },\n",
    "    \"aws\": {\n",
    "        \"upload\": True,\n",
    "        \"bucket_name\": bucket_name,\n",
    "        \"prefix\": \"fire-experiments\",\n",
    "        \"region\": aws_region,\n",
    "        \"create_bucket_if_missing\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create the directory for the config file if it doesn't exist\n",
    "config_dir = Path('../config')\n",
    "config_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save the configuration to a YAML file\n",
    "config_path = config_dir / 'fire-prediction-multi-model-config.yaml'\n",
    "with open(config_path, 'w') as f:\n",
    "    yaml.dump(multi_model_config, f, default_flow_style=False)\n",
    "\n",
    "print(f\"Multi-model configuration saved to {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Pipeline-Notebook Alignment\n",
    "\n",
    "To ensure that the pipeline training process exactly matches what we've done in this notebook:\n",
    "\n",
    "1. **Data Preprocessing**: The pipeline's `create_dataset` and `generate_features` functions should match our cleaning and feature generation steps\n",
    "\n",
    "2. **Feature Selection**: We're using these features in our models:\n",
    "   - latitude, longitude, scan, track, bright_t31, frp, confidence \n",
    "   - Plus derived features: frp_per_area, temperature_diff\n",
    "\n",
    "3. **Training Process**: \n",
    "   - Same train/test split (test_size=0.3, random_state=42)\n",
    "   - Same model types (LinearRegression, DecisionTreeRegressor) \n",
    "   - Same hyperparameters (max_leaf_nodes based on tuning)\n",
    "\n",
    "4. **Evaluation Metrics**: \n",
    "   - MAE, RMSE, R², explained variance\n",
    "\n",
    "The pipeline has been updated to follow this exact workflow to ensure consistency between interactive exploration and production deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline-Notebook Alignment Check:\n",
      "Features used: ['latitude', 'longitude', 'scan', 'track', 'bright_t31', 'frp', 'confidence', 'frp_per_area', 'temperature_diff']\n",
      "Test size: 0.3\n",
      "Random state: 1\n",
      "\n",
      "Metrics to match in pipeline:\n",
      "Linear Regression MAE: 0.0000\n",
      "Decision Tree MAE: 0.3741\n",
      "Improvement with Decision Tree: -648317472319083.00%\n"
     ]
    }
   ],
   "source": [
    "# Generate a config summary that helps ensure pipeline-notebook alignment\n",
    "alignment_check = {\n",
    "    \"notebook_features\": feature_names,\n",
    "    \"notebook_models\": {\n",
    "        \"linear_regression\": {\n",
    "            \"type\": \"LinearRegression\",\n",
    "            \"params\": {}\n",
    "        },\n",
    "        \"decision_tree\": {\n",
    "            \"type\": \"DecisionTreeRegressor\",\n",
    "            \"params\": {\"max_leaf_nodes\": best_leaf_nodes, \"random_state\": 1}\n",
    "        }\n",
    "    },\n",
    "    \"train_test_split\": {\n",
    "        \"test_size\": 0.3,\n",
    "        \"random_state\": 1\n",
    "    },\n",
    "    \"metrics\": {\n",
    "        \"linear_regression\": lr_metrics,\n",
    "        \"decision_tree\": simple_metrics\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Pipeline-Notebook Alignment Check:\")\n",
    "print(f\"Features used: {alignment_check['notebook_features']}\")\n",
    "print(f\"Test size: {alignment_check['train_test_split']['test_size']}\")\n",
    "print(f\"Random state: {alignment_check['train_test_split']['random_state']}\")\n",
    "print(\"\\nMetrics to match in pipeline:\")\n",
    "print(f\"Linear Regression MAE: {lr_metrics['mae']:.4f}\")\n",
    "print(f\"Decision Tree MAE: {simple_metrics['mae']:.4f}\")\n",
    "print(f\"Improvement with Decision Tree: {(lr_metrics['mae'] - simple_metrics['mae']) / lr_metrics['mae'] * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
